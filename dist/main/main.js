"use strict";const{app:i,BrowserWindow:g,ipcMain:s}=require("electron"),m=require("path"),P=!i.isPackaged,{exec:u,spawn:y}=require("child_process"),v=require("https"),p=require("fs"),h=require("os");let l,n;const q=()=>new Promise((t,o)=>{v.get("https://ollama.ai/install.sh",r=>{let a="";r.on("data",e=>{a+=e}),r.on("end",()=>{t(a)})}).on("error",r=>{o(r)})}),O=async()=>{const t=h.platform();if(t==="darwin"){const o=await q(),r=m.join(h.tmpdir(),"ollama-install.sh");return p.writeFileSync(r,o),p.chmodSync(r,"755"),new Promise((a,e)=>{u(`sh ${r}`,(c,d,w)=>{if(c){e(c);return}a(d)})})}else throw t==="win32"?new Error("Windows installation not yet implemented"):new Error("Unsupported platform")},x=()=>new Promise((t,o)=>{const a=h.platform()==="win32"?"ollama.exe":"ollama";n=y(a,["serve"]),n.stdout.on("data",e=>{console.log(`Ollama: ${e}`),e.toString().includes("Listening")&&t()}),n.stderr.on("data",e=>{console.error(`Ollama Error: ${e}`)}),n.on("error",e=>{o(e)})}),S=t=>new Promise((o,r)=>{const a=y("ollama",["pull",t]);a.stdout.on("data",e=>{const d=e.toString().match(/([0-9.]+)%/);if(d){const w=parseFloat(d[1]);l.webContents.send("model-pull-progress",w)}}),a.stderr.on("data",e=>{console.error(`Model Pull Error: ${e}`)}),a.on("close",e=>{e===0?o():r(new Error(`Model pull failed with code ${e}`))})});function f(){if(l=new g({width:1200,height:800,title:"Alt Text AI",icon:m.join(__dirname,"../../assets/images/icon-512.gif"),webPreferences:{nodeIntegration:!1,contextIsolation:!0,preload:m.join(__dirname,"../preload/preload.js")}}),P)l.loadURL("http://localhost:5173"),l.webContents.openDevTools();else{const t=m.join(__dirname,"../../dist/index.html");l.loadFile(t).catch(o=>{console.error("Failed to load index.html:",o),l.loadURL(`file://${t}`).catch(r=>{console.error("Failed to load using file URL:",r)})})}l.once("ready-to-show",()=>{l.show()})}s.handle("install-ollama",async()=>{try{return await O(),!0}catch(t){throw t}});s.handle("pull-model",async(t,o)=>{try{return await S(o),!0}catch(r){throw r}});s.handle("start-ollama-server",async()=>{try{return await x(),!0}catch(t){throw t}});s.handle("check-ollama-status",async()=>new Promise(t=>{u("ollama --version",o=>{t(!o)})}));s.handle("quit-app",()=>{i.quit()});i.whenReady().then(()=>{f(),i.on("activate",()=>{g.getAllWindows().length===0&&f()})});i.on("window-all-closed",()=>{process.platform!=="darwin"&&i.quit()});i.on("before-quit",()=>{n&&n.kill()});const j=()=>new Promise((t,o)=>{u("ollama --version",(r,a)=>{if(r){o(r);return}t(a.trim())})}),_=()=>new Promise((t,o)=>{const a=h.platform()==="win32"?"where ollama":"which ollama";u(a,(e,c)=>{if(e){o(e);return}t(c.trim())})}),$=()=>new Promise((t,o)=>{if(!n){t();return}n.on("close",()=>{n=null,t()}),n.kill()});s.handle("get-ollama-version",async()=>{try{return await j()}catch(t){throw t}});s.handle("get-ollama-path",async()=>{try{return await _()}catch(t){throw t}});s.handle("stop-ollama-server",async()=>{try{return await $(),!0}catch(t){throw t}});
